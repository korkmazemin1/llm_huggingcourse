{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CLASSİFİER PİPELİNE                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from transformers import pipeline\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# we choose the text classficiaton from pipeline \n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here input text\n",
    "text= \" we love breakfast\"\n",
    "\n",
    "# we can use the base modek with text then take the outputs\n",
    "outputs= classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label     score\n",
      "0  POSITIVE  0.999849\n"
     ]
    }
   ],
   "source": [
    "# convert outputs a dataframe \n",
    "df=pd.DataFrame(outputs)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER PİPELİNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\korkm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# use default for ner pipeline with simple aggregation strategy\n",
    "ner_tagger=pipeline(\"ner\",aggregation_strategy=\"simple\")\n",
    "# if u choose aggregation strategy simple you can combine the following same tag as one  \n",
    "# you can choose model with model parameter ner_tagger=pipeline(\"ner\",aggregation_strategy=\"simple\",model=bla bla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_group     score    word  start  end\n",
      "0          PER  0.951199    emin     11   15\n",
      "1          LOC  0.906955  london     28   34\n"
     ]
    }
   ],
   "source": [
    "text=\"My name is emin . I live in london and working on facebook\"\n",
    "#  make ner and take the results of ner \n",
    "output=ner_tagger(text)\n",
    "\n",
    "# visiulize the result \n",
    "results=pd.DataFrame(output)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "set_seed(0) # this command makes outputs to reproducible \n",
    "# we can use this option for testing something "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 6c0e608 (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\korkm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "generator=pipeline( \"text-generation\") \n",
    "# we can choose the model as we want on huggingface and if we not choose any of them ( not even write model parameter )\n",
    "# we chose to pipeline as text-generation\n",
    "response =\" I am sorry to hear that u order was mixed up \"\n",
    "\n",
    "prompt= \"user:\"+ text + \"Customer Service Response: \" + response\n",
    "# we give it  a sentence for the model to develope  \n",
    "outputs= generator(prompt,max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"user:My name is emin . I live in london and working on facebookCustomer Service Response:  I am sorry to hear that u order was mixed up ia do u still have this one ia can i order from my car? I know u order from my car and i ordered it from someone else!Customer Service: Hi, am i ok. Can i order the car already? I hope this is ok.\\n\\nCustomer Service:\\n\\nCustomer Service: Hi, Am i ok? Please let me know what the problems of the order is, it's wrong. ia have not ordered my car yet so\"}]\n"
     ]
    }
   ],
   "source": [
    "print(outputs)# as u can see in the ouputs model has developed to customer service response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'there was a lion on top of it\")\\n\\n- No-one seemed to know about it.\\n\\n- I\\'ll go back to my drawing, now, because I\\'ve decided to make an \"I can\\'t even guess\" puzzle from scratch, but it won\\'t be that hard, because once you get into the world that I\\'ll put together, you\\'ll have fun playing with my game. And the puzzles don\\'t let you draw a lot. (Actually it should!)\\n\\n- I\\'ve realized that I\\'m missing the main thing. My body doesn\\'t seem to function pretty.\\n\\n- I\\'m still trying'}]\n"
     ]
    }
   ],
   "source": [
    "prompt=\"there was a lion\"\n",
    "outputs= generator(prompt, max_length=128)\n",
    "# the default model gpt 2 is bettter for develop text from one sentence that is what we try now \n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSLATİON PİPELİNE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebook un alanı indirmeye izin vermediğinden başka bir py dosyasında işleme devam ediliyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision 686f1db (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af110a229f564a1c98240dc410efded3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   1%|1         | 10.5M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/t5-base/a90903540cc02cbeb7ff9f823f1a80eb778c7e22426a0e620b01c77a5ec8f5b4?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1731428698&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMTQyODY5OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby90NS1iYXNlL2E5MDkwMzU0MGNjMDJjYmViN2ZmOWY4MjNmMWE4MGViNzc4YzdlMjI0MjZhMGU2MjBiMDFjNzdhNWVjOGY1YjQ%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=cDzi-es8Pb7Ve6hEXlIVwE5yG2x72AFZD1FJG0PbnKlCqhDtO8njxLV9UZiurq-jtwQzSYPpmPlLk90D4SCcDf-wUlQY7RPqXeSdKR7HIM3UPqR3aTz59Z0FTADZXJYwbDfUh8OHkhTIkTnms8jvc%7EO%7Ep8xjxtJlBYTvnidLwsm4XFqd4WVh5d--uQMRZ2-fLZRloISNCqG%7EW2T0lbjU2YYaU742k8jHnUXnsHZO0daZsBYP5IXAHiHpdcY2gQ1AzlTX-wFqdH3wZNk3GNi9LEz2-zQ3gpriT8k-hnnu3kFNfK31jSrVIF4rOIuKrzpmvbohvFhkvAHOHjvEbMFs3g__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d12642db994b6eb29729d21055710f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  56%|#####6    | 503M/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f0701b49d1449199dbb6d50013a95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312c23bef30642f4beabc221567daf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f861e1699c4539982bbf48933ad412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\korkm\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translator=pipeline(\"translation_en_to_de\") # we chose to language for translation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
